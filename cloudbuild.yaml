substitutions:
  _PROJECT_ID: "rugged-precept-451103-n9"
  _BUCKET: "dataflow-test-streaming"          # For storing template files
  _TEMPLATE_NAME: "my-streaming-template"
  _REPO_NAME: "my-docker-repo"
  _IMAGE_NAME: "dataflow-pipeline"
  _REGION: "us-central1"
  _SERVICE_ACCOUNT: "1046723826220-compute@developer.gserviceaccount.com"
  _AGG_WINDOW: "10"
  _ALERT_WINDOW: "1"
  _INPUT_TOPIC: "projects/rugged-precept-451103-n9/topics/streaming-events"
  _ALERT_TOPIC: "projects/rugged-precept-451103-n9/topics/high-volume-alerts"
  _OUTPUT_TABLE: "rugged-precept-451103-n9:dataflow_test.purchase_summary"


steps:
  # 1) Build Docker image containing your pipeline code
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: bash
    args:
      - -c
      - |
        set -e
        echo "Checking for existing Artifact Registry repo..."
        if ! gcloud artifacts repositories describe "${_REPO_NAME}" --location="${_REGION}" >/dev/null 2>&1; then
          echo "Repo '${_REPO_NAME}' not found. Creating..."
          gcloud artifacts repositories create "${_REPO_NAME}" \
            --repository-format=docker \
            --location="${_REGION}" \
            --description="My Docker repo (auto-created in Cloud Build)"
        else
          echo "Repo '${_REPO_NAME}' already exists."
        fi
  - name: 'gcr.io/cloud-builders/docker'
    args:
      [
        'build', '--no-cache' ,
        '-t', 'us-central1-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}',
        '.'
      ]

  # 2) Push it to Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args:
      [
        'push',
        'us-central1-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}'
      ]

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'gcloud'
    args:
      [
      'dataflow',
      'flex-template',
      'build',
      'gs://${_BUCKET}/templates/${_TEMPLATE_NAME}.json',
      '--image',
      'us-central1-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}',
      '--sdk-language',
      'PYTHON',
      '--metadata-file',
      'metadata.json'
      ]

# Run the Dataflow job using the Flex Template (Step 4)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - -c
      - |
        set -e
        gcloud dataflow flex-template run "streaming-purchase-$(date +%Y%m%d%H%M%S)" \
        --project="${_PROJECT_ID}" \
        --region="${_REGION}" \
        --template-file-gcs-location="gs://${_BUCKET}/templates/${_TEMPLATE_NAME}.json" \
        --service-account-email="${_SERVICE_ACCOUNT}" \
        --temp-location=gs://${_BUCKET}/temp \
        --staging-location=gs://${_BUCKET}/staging \
        --parameters=alert_window_sec="${_ALERT_WINDOW}",\
        aggregation_window_sec="${_AGG_WINDOW}",\
        input_topic="${_INPUT_TOPIC}",\
        alert_topic="${_ALERT_TOPIC}",\
        output_table="${_OUTPUT_TABLE}" 
        

timeout: 900s
options:
  logging: CLOUD_LOGGING_ONLY

